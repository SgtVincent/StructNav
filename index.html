<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	.abstract {
		font-size: 16px;
		line-height: 1.2em;
	}

	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	imgcaption {
		margin-top: 10px;
		font-size: 14px;
		color: #888;
	}


	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 200px;
		text-align: center;
		font-size: 22px;
	}

	pre {
		display: block;
		padding: 8px;
		margin: 0 0 5px;
		font-size: 12px;
		line-height: 1.42857143;
		color: #333;
		word-break: break-all;
		word-wrap: break-word;
		background-color: #f5f5f5;
		border: 1px solid #ccc;
		border-radius: 4px;
	}

	pre code {
		display: block;
		padding: 5px;
		font-family: Consolas, Monaco, 'Andale Mono', monospace;
		box-sizing: border-box;
	  }

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers</title>
	<meta property="og:image" content="resources/intuition.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title"
		content="How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers" />
	<meta property="og:description"
		content="Project webpage for paper 'How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers' published on RSS 2023" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with
			Semantic Frontiers</span>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://sgtvincent.github.io/">Junting
									Chen*</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://ghli.org/">Guohao Li*</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://suryanshkumar.github.io/">Suryansh
									Kumar</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://www.bernardghanem.com/">Bernard
									Ghanem</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://www.yf.io/">Fisher Yu</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='http://arxiv.org/abs/2305.16925'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a
									href='https://www.youtube.com/watch?v=aZYc2HjlnzQ&ab_channel=JuntingChen'>[Video]</a></span>
						</center>
					</td>
					<td align=center width=240px>
						<center>
							<span style="font-size:24px"><a href='index.html'>[GitHub (coming soon)]</a></span><br>
						</center>
					</td>

				</tr>
			</table>
		</table>
	</center>

	<!-- Add a sub-title here -->
	<br>
	<center>
		<span style="font-size:30px">Published on RSS 2023</span>
	</center>

	<br>

	<!-- Teaser image -->
	<center>
		<table align=center width=800px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1200px" src="./resources/teaser.gif" />
					</center>
					<imgcaption>
						<em>With semantic reasoning on the partial scene reconstruction, our agent can leverage the
							general scene layout priors and predict the possible target locations. As a result, our
							agent can navigate to the target object with shorter paths in unseen environments.</em>
					</imgcaption>
				</td>
			</tr>
		</table>

	</center>



	<table align=center width=850px>
		<tr>
			<td>

			</td>
		</tr>
	</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center>
			<h1>Abstract</h1>
		</center>
		<!-- insert abstract text here -->
		<tr>
			<td>

				<div class="abstract">
					<b>Summary:</b> This work presnets a modular and training-free system to tackle the object goal
					navigation problem.
					The system builds a structured scene representation during active exploration, propagates semantics
					in the scene graphs to infer the target location, and
					introduces those semantics to the geometric frontiers. With semantic frontiers, the agent navigate
					to the most promising areas to search for the goal object and avoid detours in unseen environments.
				</div>


				<!-- <div class="abstract">
					Object goal navigation is an important problem in Embodied AI that involves guiding the agent to
					navigate to an instance of the object category in an unknown environment---typically an indoor scene.
					Unfortunately, current state-of-the-art methods for this problem rely heavily on data-driven approaches,
					e.g., end-to-end reinforcement learning, imitation learning, and others. Moreover, such methods are	
					typically costly to train and difficult to debug, leading to a lack of transferability and
					explainability. Inspired by recent successes in combining classical and learning methods, we present a
					modular and training-free solution, which embraces more classic approaches, to tackle the object goal
					navigation problem. Our method builds a structured scene representation based on the classic visual
					simultaneous localization and mapping (V-SLAM) framework. We then inject semantics into geometric-based
					frontier exploration to reason about promising areas to search for a goal object. Our structured scene
					representation comprises a 2D occupancy map, semantic point cloud, and spatial scene graph.
				</div>

				<div class="abstract">
					Our method propagates semantics on the scene graphs based on language priors and scene statistics to
					introduce semantic knowledge to the geometric frontiers. With injected semantic priors, the agent can
					reason about the most promising frontier to explore. The proposed pipeline shows strong experimental
					performance for object goal navigation on the Gibson benchmark dataset, outperforming the previous
					state-of-the-art. We also perform comprehensive ablation studies to identify the current bottleneck in
					the object navigation task.
				</div> -->

			</td>
		</tr>

		<!-- insert image here -->
		<tr>
			<td>
				<center>
					<img class="round" style="width:800px" src="./resources/intuition.png" />
				</center>
				<imgcaption>
					<em>Intuition of this work. ObjectNav can be decomposed into 1) semantic questiosn: inferring the
						potential position
						of the target object in the scene and 2) geometric question: point-to-point navigation. These two questions can be solved by semantic reasoning and classic planning separately on different representations.
				</imgcaption>
			</td>
		</tr>

	</table>
	<br>

	<hr>
	<center>
		<h1>Video</h1>
	</center>
	<p align="center">
		<!-- <iframe width="660" height="395" src="https://www.youtu.com/embed/aZYc2HjlnzQ" frameborder="0" allowfullscreen></iframe> -->
		<iframe width="800" height="450" src="https://www.youtube.com/embed/aZYc2HjlnzQ" title="YouTube video player"
			frameborder="0"
			allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
			allowfullscreen></iframe>
	</p>
	<hr>
	<!-- put link to slides here  -->

	<!-- <table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	<hr> -->

	<!-- <center>
		<h1>Code</h1>
	</center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table> -->
	<center>
		<h1>Method</h1>
	</center>

	<table align=center width=400px>
		<tr>
			<center>
				<img class="round" style="width:800px" src="./resources/method_overview.png" /></td>
			</center>
		</tr>
	</table>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					The proposed method builds a structured scene representation on the fly, which consists of a semantic
point cloud, a 2D occupancy map, and a spatial scene graph. Then the scene graph is used to propagate semantics to the geometric frontiers. 
With semantic frontiers, the agent can navigate to the most promising areas to search for the goal object and avoid detours in unseen environments.

				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px">&nbsp;<a href='index.html'>[Code (coming soon)]</a>
			</center>
			</span>
	</table>
	<br>
	<hr>
	<table align=center width=800px>
		<center>
			<h1>Paper and Bibtex</h1>
		</center>
		<tr style="height:250px">
			<td><a href="https://arxiv.org/pdf/2305.16925.pdf"><img class="layered-paper-big" style="height:175px"
						src="./resources/paper_front_page.png" /></a></td>
			<td><span style="font-size:12pt">Junting Chen, Guohao Li, Suryansh Kumar, Bernard Ghanem, Fisher Yu.<br>
					<b>How To Not Train Your Dragon: Training-free Embodied <br> Object Goal Navigation with Semantic
						Frontiers.</b><br>
					In Conference Robotics: Science and Systems, 2023.<br>
					(hosted on <a href="http://arxiv.org/abs/2305.16925">ArXiv</a>)
					<br>
					<span style="font-size:4pt"><a href=""><br></a>
					</span>
			</td>
		</tr>
		<!-- add a code block for bibtex -->
		<tr>
			<td colspan="2">
			</td> 
		</tr>
		
		<tr>
			<td colspan="2">
			<pre >
				<code>
	@misc{chen2023train,
		title={How To Not Train Your Dragon: Training-free Embodied Object Goal Navigation with Semantic Frontiers}, 
		author={Junting Chen and Guohao Li and Suryansh Kumar and Bernard Ghanem and Fisher Yu},
		year={2023},
		eprint={2305.16925},
		archivePrefix={arXiv},
		primaryClass={cs.CV}
	}
				</code>
			</pre>
			</td>
		</tr>
	</table>
	<br>


	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a
						href="http://richzhang.github.io/">Richard Zhang</a> for a <a
						href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found
					<a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

	<br>
</body>

</html>